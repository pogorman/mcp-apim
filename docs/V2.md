# V2: Real Estate Transfer Data — The Game Changer

## What Changed

V2 adds Philadelphia's **Real Estate Transfer Tax (RTT) dataset** — 5.05 million records of every property deed, mortgage assignment, sheriff sale, and transfer recorded in the city. This was the single biggest gap in our investigative capability.

## Why It Matters

The original notebooks identified **19 analytical factors** for scoring poverty profiteering risk. Three of the most important ones were impossible without transfer data:

| Factor | What It Detects | V1 | V2 |
|--------|----------------|-----|-----|
| **F10 — Sheriff Sale Purchases** | LLCs buying distressed properties at sheriff sales | Impossible | `search_transfers(documentType="SHERIFF")` |
| **F19 — Dollar Transfers** | $1 LLC-to-LLC ownership shuffles that hide true owners | Impossible | `search_transfers(maxConsideration=1)` |
| **F13 — Home Value Appreciation** | Buy low, sell high flipping patterns | Partial (assessments only) | Full transfer price history per property |

## V1 vs V2 Comparison

| Metric | V1 | V2 |
|--------|-----|-----|
| **Tables** | 10 | 11 (+rtt_summary) |
| **Total rows** | ~29M | ~34M (+5.05M) |
| **MCP tools** | 12 | 14 (+get_property_transfers, +search_transfers) |
| **API endpoints** | 12 | 14 |
| **Data size** | ~4.4GB | ~5.5GB |
| **Transfer records** | 0 | 5,050,000+ |
| **Data source** | Static CSV exports | Static + Carto API pipeline |

## New Capabilities

### 1. Property Transfer History (`get_property_transfers`)
For any property, see the complete chain of ownership: who sold to whom, for how much, when, and what type of document (deed, sheriff deed, mortgage assignment, etc.).

**Example:** "Show me the transfer history for 405100505"

### 2. Transfer Search (`search_transfers`)
Search across 5M+ records by grantor/grantee name, document type, zip code, or sale price range. The killer feature: filter by `maxConsideration=1` to find all $1 transfers.

**Examples:**
- "Find all $1 property transfers in 19134"
- "Show me all sheriff sale purchases by GEENA LLC"
- "Find transfers where consideration was $0-$1 and document type is DEED"

### 3. Enhanced Property Profiles
`get_property_profile` now includes `transfer_count` in its response — immediately see how many times a property has changed hands.

### 4. Enhanced Area Stats
`get_area_stats` now includes `transfer_stats` for each zip code:
- Total transfers
- Sheriff sales count
- $1 transfer count
- Average sale price

### 5. Carto Download Pipeline
New `sql/download-carto.js` script downloads live data from the Philadelphia public Carto API. This is a reusable pipeline that can be extended to refresh any of the 10 existing tables or add the 2 remaining datasets (violations, permits).

## New Data: rtt_summary Table

| Column | Type | Purpose |
|--------|------|---------|
| `objectid` | INT (PK) | Unique record ID |
| `document_id` | VARCHAR | Document identifier |
| `document_type` | VARCHAR | DEED, SHERIFF DEED, MORTGAGE, etc. |
| `display_date` | DATETIME2 | Document/recording date |
| `street_address` | NVARCHAR | Property address |
| `zip_code` | VARCHAR | Zip code |
| `grantors` | NVARCHAR | Seller/transferor name(s) |
| `grantees` | NVARCHAR | Buyer/transferee name(s) |
| `total_consideration` | DECIMAL | Total sale price ($1 = suspicious) |
| `cash_consideration` | DECIMAL | Cash portion |
| `assessed_value` | DECIMAL | Assessed value at time of transfer |
| `fair_market_value` | DECIMAL | Fair market value |
| `opa_account_num` | VARCHAR | **Join key** to opa_properties |
| `recording_date` | VARCHAR | When recorded |
| `legal_remarks` | NVARCHAR | Legal notes |

Plus tax columns (state/local amounts and percentages), receipt info, condo details, and discrepancy flags. 31 columns total (trimmed from 50 — skipped geometry, adjusted duplicates, and parsed address components).

**7 indexes** optimized for: OPA lookups, grantor/grantee search, document type filtering, date ordering, consideration amount filtering, and zip code queries.

## The Data Journey: V1 to V2

### V1 — Static CSV Exports (Sessions 1-25)

The project started with 10 static CSV files from the [PhillyStats](https://github.com/davew-msft/PhillyStats) repository. These were one-time exports from Philadelphia's open data portals — snapshots in time, not live data. The original Jupyter notebooks used the Philadelphia Carto SQL API to query data interactively, but we chose to bulk-load CSVs into Azure SQL for production performance.

This gave us 29 million rows across 10 tables: properties, entities, entity-address links, code violations, assessments, business licenses, commercial activity licenses, appeals, and demolitions. Powerful — but with two blind spots:

1. **No transfer/sale data** — We couldn't track who sold what to whom, for how much, or detect $1 transfers and sheriff sales
2. **Stale data** — The CSVs were static. The Carto API had 428,000+ new violation records we were missing

### The Discovery (Session 26)

We went back to the original Jupyter notebooks and compared what they query on the Carto API vs what we had loaded. The discovery:

| Missing Dataset | Rows | Why It Matters |
|----------------|------|---------------|
| **rtt_summary** | **5.05M** | Real estate transfers — the game changer |
| violations | 1.95M | Individual violation codes (more granular than case_investigations) |
| permits | 907K | Building permits, renovation tracking |

The `rtt_summary` dataset enables three of the notebook's 19 profiteering factors that were previously impossible: sheriff sale detection, $1 transfer detection, and appreciation analysis via actual sale prices.

### V2 — Live Carto Pipeline + Transfer Data

We built `sql/download-carto.js` — a Node.js script that downloads data directly from the Philadelphia Carto SQL API (`https://phl.carto.com/api/v2/sql`). This is the same public API the Jupyter notebooks use, but automated for bulk download:

1. **Query the ID range**: `SELECT MIN(cartodb_id), MAX(cartodb_id) FROM rtt_summary`
2. **Paginate in 50K batches**: `WHERE cartodb_id BETWEEN <start> AND <start + 50000>`
3. **Write to CSV**: With proper escaping for fields containing commas, quotes, and newlines
4. **Retry logic**: Exponential backoff on 429 rate limit errors, 200ms courtesy delay between batches

The result: 5,047,081 rows downloaded in ~5 minutes (1.2GB CSV), then bulk-loaded to Azure SQL in ~60 minutes using the existing TDS bulk copy protocol.

This pipeline is reusable — adding the `violations` and `permits` datasets in V3 is just adding their column configs to the `TABLES` object in `download-carto.js`. And it can refresh all existing tables to get the latest data.

## Data Source

**Philadelphia Carto API** — `https://phl.carto.com/api/v2/sql`

Public, no authentication required, updated daily. Download script uses the same pagination pattern as the original PhillyStat Jupyter notebooks: query min/max `cartodb_id`, then paginate with `BETWEEN` clauses in 50,000-row batches.

## Files Added/Changed

### New Files
| File | Purpose |
|------|---------|
| `sql/download-carto.js` | Carto API downloader — batch download with retry logic |
| `functions/src/functions/getPropertyTransfers.ts` | GET `/properties/{parcelNumber}/transfers` |
| `functions/src/functions/searchTransfers.ts` | POST `/search-transfers` |
| `docs/V2.md` | This file |

### Modified Files
| File | Changes |
|------|---------|
| `sql/schema.sql` | Added `rtt_summary` table (31 columns) + 7 indexes |
| `sql/bulk_import.js` | Added rtt_summary table config + rowMapper |
| `functions/src/functions/getPropertyProfile.ts` | Added `transfer_count` to counts query |
| `functions/src/functions/getAreaStats.ts` | Added transfer stats (sheriff sales, $1 transfers, avg price) |
| `mcp-server/src/apim-client.ts` | Added `getPropertyTransfers()` and `searchTransfers()` |
| `mcp-server/src/tools.ts` | Added 2 MCP tools, updated `run_query` table list |
| `mcp-server/src/tool-executor.ts` | Added 2 tools to TOOLS array + executeTool + updated SYSTEM_PROMPT |
| `m365-agent/ai-plugin.json` | Added 2 tools to function list |
| `m365-agent/declarativeAgent.json` | Updated instructions + 2 new conversation starters |
| `sk-agent/Plugins/OwnerPlugin.cs` | Added `GetPropertyTransfers` |
| `sk-agent/Plugins/AreaPlugin.cs` | Added `SearchTransfers` |
| `CLAUDE.md` | Updated all counts (tables, rows, tools, endpoints) |
| `docs/ELI5.md` | Added transfer data explanation |
| `docs/ARCHITECTURE.md` | Added endpoints, tools, table |
| `docs/USER_GUIDE.md` | Added transfer query examples |

## What's Next (Future V3)

Two more Carto datasets are available but not yet loaded:

| Dataset | Rows | What It Adds |
|---------|------|-------------|
| `violations` | 1.95M | Individual violation codes (more granular than case_investigations) |
| `permits` | 907K | Building permits, contractor info, renovation tracking |

The `download-carto.js` pipeline is ready — just add their column configs to the `TABLES` object.

Also available: refresh all 10 existing tables from Carto to get the latest data (case_investigations alone has 428K new rows since our last load).

## Deployment Checklist

1. Download data: `node sql/download-carto.js rtt_summary`
2. Create SQL table: Run CREATE TABLE + CREATE INDEX from schema.sql
3. Bulk import: `cd functions && node ../sql/bulk_import.js`
4. Build functions: `npm run build -w functions`
5. Deploy functions (staging dir pattern)
6. Build MCP server: `npm run build -w mcp-server`
7. Deploy container app: `bash infra/deploy-agent.sh`
8. Rebuild SK agent container
9. Re-upload M365 agent package
10. Deploy SWA: `bash infra/deploy-swa.sh`
